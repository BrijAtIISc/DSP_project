{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJr2LkmdYX4B"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jP2eLoGYX4D"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hn7uoCP5Yo0F",
    "outputId": "40e29d11-e4e0-4452-db89-c76a007860dc"
   },
   "outputs": [],
   "source": [
    "%pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G7hDBy6YX4D"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from newspaper import Article, Config\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ha47y9k0YX4E"
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "# GCSJ_API_KEY = os.environ.get('GCSJ_API_KEY')\n",
    "# GCSJ_ENGINE_ID = os.environ.get('GCSJ_ENGINE_ID')\n",
    "GCSJ_API_KEY = \"AIzaSyDaP40-ZAjvFanRhgvRFwMXGOd_wdSJyCI\"\n",
    "GCSJ_ENGINE_ID = \"83b96784e08f54c33\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8ejA8gqYX4F"
   },
   "source": [
    "## Data Locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRt4g8DcYX4F"
   },
   "outputs": [],
   "source": [
    "def generate_weekdays(num_days, offset=0):\n",
    "    dt = datetime.today() - timedelta(days=offset-1)\n",
    "    while num_days > 0:\n",
    "        dt -= timedelta(days=1)\n",
    "        if dt.weekday() < 5:\n",
    "            num_days -= 1\n",
    "            yield dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OnHZCB4YX4F"
   },
   "outputs": [],
   "source": [
    "def fetch_google_results(query):\n",
    "    search_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    results = []\n",
    "    for offset in [1, 11, 21]:\n",
    "        params = {\n",
    "            \"key\": GCSJ_API_KEY,\n",
    "            \"cx\": GCSJ_ENGINE_ID,\n",
    "            \"start\": offset,\n",
    "            \"dateRestrict\": 'd1',\n",
    "            \"lr\": 'lang_en',\n",
    "            \"gl\": 'in',\n",
    "            \"num\": 10,\n",
    "            \"q\": query,\n",
    "        }\n",
    "        res = requests.get(search_url, params=params)\n",
    "        res.raise_for_status()\n",
    "        result = res.json().get(\"items\", [])\n",
    "        results.extend(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6GcTm46YX4F"
   },
   "outputs": [],
   "source": [
    "def get_todays_news():\n",
    "    dt = datetime.now()\n",
    "    today = dt.strftime('%Y-%m-%d')\n",
    "    tomorrow = (dt + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    query = f'stock market summary today india \"nifty\" \"sensex\" -site:youtube.com after:{today} before:{tomorrow}'\n",
    "    items = fetch_google_results(query)\n",
    "    results = []\n",
    "\n",
    "    for item in items:\n",
    "        date_pattern = r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}'\n",
    "        if matches := re.findall(date_pattern, json.dumps(item)):\n",
    "            item['time'] = datetime.fromisoformat(max(matches)).isoformat()\n",
    "        obj = {\n",
    "            'time': item.get('time', None),\n",
    "            'link': item.get('link', None),\n",
    "        }\n",
    "        results.append(obj)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['time'] = pd.to_datetime(results_df['time'])\n",
    "    results_df.to_csv(\"../Dataset/scraper/raw/search_results.csv\", sep='|', index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "get_todays_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTjuwikcYX4F"
   },
   "source": [
    "## Data Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p32crnSLYX4F"
   },
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = Config()\n",
    "    config.request_timeout = 10\n",
    "    config.memoize_articles = False\n",
    "    config.fetch_images = False\n",
    "    config.browser_user_agent = random.choice([\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
    "    ])\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MalD08NYX4G"
   },
   "outputs": [],
   "source": [
    "class CustomArticle(Article):\n",
    "    def build(self):\n",
    "        super().build()\n",
    "        soup = BeautifulSoup(self.html, 'html.parser')\n",
    "\n",
    "        if not self.authors:\n",
    "            candidates = map(lambda x: x.get_text().strip(), soup.select('a[href*=author]'))\n",
    "            self.authors.extend(candidates)\n",
    "\n",
    "        if not self.text:\n",
    "            paragraphs = soup.find_all('p')\n",
    "            lists = soup.find_all('ul')\n",
    "            divs = soup.find_all('div')\n",
    "\n",
    "            para_text = \"\\n\".join([p.get_text().strip() for p in paragraphs])\n",
    "            list_text = \"\\n\".join([ul.get_text().strip() for ul in lists])\n",
    "            divs_text = \"\\n\".join([d.get_text().strip() for d in divs])\n",
    "\n",
    "            self.text = para_text + \"\\n\" + list_text + \"\\n\" + divs_text\n",
    "            self.text = self.text.strip()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def datetime(self):\n",
    "        date_pattern = r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}'\n",
    "        json_str = json.dumps(self.meta_data)\n",
    "        if matches := re.findall(date_pattern, json_str):\n",
    "            latest_date = datetime.fromisoformat(max(matches))\n",
    "            return latest_date.isoformat()\n",
    "\n",
    "        date_patterns = [\n",
    "            (\"%B %d, %Y, %H:%M\", r\"\\b\\w+ \\d{1,2}, \\d{4}, \\d{2}:\\d{2}\"),\n",
    "            (\"%b %d, %Y, %H:%M\", r\"\\b\\w{3} \\d{1,2}, \\d{4}, \\d{2}:\\d{2}\"),\n",
    "            (\"%b %d, %Y %H:%M\", r\"\\b\\w{3} \\d{1,2}, \\d{4} \\d{2}:\\d{2}\"),\n",
    "            (\"%d %b %I:%M %p\", r\"\\b\\d{1,2} \\w{3} \\d{1,2}:\\d{2} (?:am|pm)\"),\n",
    "            (\"%H:%M (IST) %d %b %Y\", r\"\\d{2}:\\d{2} \\(IST\\) \\d{1,2} \\w{3} \\d{4}\"),\n",
    "        ]\n",
    "\n",
    "        for fmt, rgx in date_patterns:\n",
    "            for match in re.finditer(rgx, self.html, re.IGNORECASE):\n",
    "                substring = match.group(0)\n",
    "                try:\n",
    "                    parsed_date = datetime.strptime(substring, fmt)\n",
    "                    if parsed_date.year < 2000:     parsed_date = parsed_date.replace(year=datetime.now().year)\n",
    "                    return parsed_date.isoformat()\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S8UD19IYX4G"
   },
   "outputs": [],
   "source": [
    "def scrape_article(obj):\n",
    "    try:\n",
    "        time.sleep(random.uniform(0, 2))\n",
    "        article = CustomArticle(obj['link'], config=get_config())\n",
    "        article.build()\n",
    "        return {\n",
    "            \"url\": obj['link'],\n",
    "            \"source_url\": article.source_url,\n",
    "            \"title\": article.title,\n",
    "            \"text\": article.title+\"\\n\"+article.text,\n",
    "            \"metadata\": article.meta_data,\n",
    "            \"datetime\": obj['time'] if not pd.isna(obj['time']) else article.datetime,\n",
    "            \"authors\": article.authors,\n",
    "            \"description\": article.meta_description,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {obj['link']}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObB-iIX0YX4G"
   },
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "results_df = pd.read_csv('../Dataset/scraper/raw/search_results.csv', sep='|', parse_dates=['time']).fillna('')\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sTDGatyYX4G"
   },
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "data = results_df.apply(scrape_article, axis=1).tolist()\n",
    "data = [item for item in data if item is not None]\n",
    "with open(f'../Dataset/scraper/raw/rawdata_{datetime.now().timestamp()}.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2jP2eLoGYX4D",
    "c8ejA8gqYX4F",
    "nTjuwikcYX4F"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
